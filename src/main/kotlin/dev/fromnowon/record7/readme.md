# PromptCache

讲人话就是询问一些重复性的问题，没必要每次都去请求 LLM API，可以直接从缓存中获取